{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extern crate rand;\n",
    "use rand::Rng;\n",
    "use std::env;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// The number of inputs for this Perceptron\n",
    "const INPUTS: usize = 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// This object will hold the inputs for the Perceptron\n",
    "struct PerceptronInputs {\n",
    "    values: [f64; INPUTS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// A Perceptron is the lowest level unit of an artificial neural network. \n",
    "/// The Perceptron coded here has 2 inputs and can be used to solve 2-variable classification problems,\n",
    "/// in this case to solve a straight line linear equation in the form y = mx + b.\n",
    "/// Writing an algorithm to do this is trivially easy, but the Perceptron will be trained to solve it,\n",
    "/// and will then be tested to confirm a correct solution.\n",
    "/// \n",
    "/// The bias property corresponds to the b constant, which in this example is interpreted as the y intercept.\n",
    "/// The learning_rate property determine how much to change the weights are each training cycle.\n",
    "/// The weights will be used to determine the importance of each input. \n",
    "/// weight[0] is associated with the x variable, and weight[1] with the y variable.\n",
    "#[derive(Debug)]\n",
    "struct Perceptron {\n",
    "    bias: f64,\n",
    "    learning_rate: f64,\n",
    "    weights: [f64; INPUTS],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "impl Perceptron {\n",
    "    /// This function returns the predicted result. In a neural network, this\n",
    "    /// would be a sigmoid or some similar differentiable function that returns a value in the range (0,1).\n",
    "    /// For a standalone Perceptron, it is sufficient to use a simple step function\n",
    "    /// that returns 0 or 1, by comparing the incoming value+bias to 0.\n",
    "    /// # Arguments\n",
    "    ///\n",
    "    /// * value: sum of the weighted inputs\n",
    "    /// \n",
    "    /// # Returns\n",
    "    ///\n",
    "    /// * 0 if the sum is less than 0, otherwise returns 1\n",
    "    ///\n",
    "    fn activate(&self, value: f64) -> i32 {\n",
    "        // s is the sigmoid function, in python, in case you want to try it out\n",
    "        // return 0 if (1 / (1 + math.exp(-1 * (value+self.bias)))) < .5 else 1\n",
    "\n",
    "        // simple stepwise will work fine for a simple standalone Perceptron\n",
    "        // let result = value + self.bias;\n",
    "        // let separator = 0.0;\n",
    "        \n",
    "        // calculate the result based on the sigmoid function\n",
    "        let result = 1.0 / (1.0 + (-1.0 * (value + self.bias)).exp());\n",
    "        let separator = 0.5;\n",
    "\n",
    "        if  result < separator { \n",
    "            0\n",
    "        } else {\n",
    "            1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    /// The perceptron can be tested by providing training data but no expected result.\n",
    "    /// Each raw input is modified by the weight for that input, then the sum of the inputs\n",
    "    /// is passed to the activation function. The activation function decides how to classify the value.\n",
    "    /// # Arguments\n",
    "    ///\n",
    "    /// * `perceptron` - the Perceptron to be queried\n",
    "    /// * `perceptron_inputs` - object that contains an array of values, one for each Perceptron input\n",
    "    /// \n",
    "    /// # Return\n",
    "    ///\n",
    "    /// * The result of the activation function\n",
    "    fn query(&mut self, perceptron_inputs: &PerceptronInputs) -> i32 {\n",
    "        let mut weighted_sum: f64 = 0.0;\n",
    "        for i in 0..INPUTS {\n",
    "            weighted_sum += self.weights[i] * perceptron_inputs.values[i];\n",
    "        }\n",
    "        self.activate(weighted_sum)\n",
    "    }    \n",
    "    \n",
    "    \n",
    "    /// The perceptron can be trained by providing training data and an expected result. If it guesses\n",
    "    /// wrong, the weight for each input will be adjusted.\n",
    "    ///\n",
    "    /// # Arguments\n",
    "    ///\n",
    "    /// * `perceptron` - the perceptron to be trained\n",
    "    /// * `perceptron_inputs` - object that contains an array of values, one for each Perceptron input\n",
    "    ///\n",
    "    /// # Returns \n",
    "    /// \n",
    "    /// * the query result\n",
    "    fn train(&mut self, perceptron_inputs: &PerceptronInputs, target: i32) -> i32 {\n",
    "        let result = self.query(perceptron_inputs);\n",
    "        if target != result {\n",
    "            // the result was incorrect. Modify the weights according to whether 0 or 1 was expected\n",
    "            // Change the weights according to the value * learning rate.\n",
    "            let delta64: f64 = f64::from(target - result);\n",
    "            for i in 0..INPUTS {\n",
    "                self.weights[i] += delta64 * perceptron_inputs.values[i] * self.learning_rate;\n",
    "            }\n",
    "            self.bias += delta64 * self.learning_rate;\n",
    "        }\n",
    "        result\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Returns an initialized Perceptron\n",
    "/// \n",
    "/// # Arguments\n",
    "///\n",
    "/// * `bias` - offset for the activation function\n",
    "/// * `learning_rate`: how much to change the weights when a prediction is incorrect\n",
    "/// \n",
    "/// # Returns\n",
    "/// \n",
    "/// * the initialized Perceptron\n",
    "fn init_perceptron(bias: f64, learning_rate: f64) -> Perceptron {\n",
    "\n",
    "    let mut rng = rand::thread_rng();\n",
    "    let mut perceptron = Perceptron {\n",
    "        bias: bias,\n",
    "        learning_rate: learning_rate,\n",
    "        weights: [0.0; INPUTS],\n",
    "    };\n",
    "    // the weights are initialized to a random value in the range [0.01..1)\n",
    "    for i in 0..INPUTS {\n",
    "        perceptron.weights[i] = rng.gen_range(0.0..0.99)+0.01; \n",
    "    }    \n",
    "    perceptron\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "/// This function creates a perceptron and solves a linear inequality\n",
    "/// in the form ax + by + c > 0\n",
    "///\n",
    "/// # Arguments\n",
    "///\n",
    "/// * `x_coefficient` - the coefffient of x (a)\n",
    "/// * `y_coefficient` - the coefffient of y (b)\n",
    "/// * `constant_term` - the constant value (c)\n",
    "pub fn run(x_coefficient: f64, y_coefficient: f64, constant_term: f64 ) {\n",
    "    \n",
    "    let mut rng = rand::thread_rng();\n",
    "    \n",
    "    // bias should be initialized to a near-zero value\n",
    "    let bias: f64 = 0.01;\n",
    "    // learning rate should be low enough to avoid overcorrecting, but smaller values\n",
    "    // will take longer to converge\n",
    "    let learning_rate: f64 = 0.0005;\n",
    " \n",
    "    let mut perceptron = init_perceptron(bias, learning_rate);\n",
    "    println!(\"Initialized, before training {:?}\", perceptron);\n",
    "    \n",
    "    // train the Perceptron with a large number of random x and y variable values\n",
    "    // we hope that it will eventually determine the a, b, and c values of the inequality\n",
    "    // ax + by + c > 0\n",
    "    for _ in 0..10000 {\n",
    "        // The test range matters. Too small or too large a range \n",
    "        // will affect the accuracy of the results\n",
    "        let x = rng.gen_range(-10.0..10.0);\n",
    "        let y = rng.gen_range(-10.0..10.0);\n",
    "        let mut perceptron_inputs = PerceptronInputs { values: [0.0; INPUTS] };\n",
    "        perceptron_inputs.values[0] = x;\n",
    "        perceptron_inputs.values[1] = y;\n",
    "        //  we have to know the intended result in order to perform the training\n",
    "        let mut target = 0;\n",
    "        if x_coefficient * x + y_coefficient * y - constant_term > 0.0 {\n",
    "            target = 1;\n",
    "        }\n",
    "        perceptron.train(&perceptron_inputs, target);\n",
    "    }\n",
    "    println!(\"After training {:?}\", perceptron);\n",
    "    \n",
    "    // normalize the actual and calculated inequalities in y = mx + b format for intuitive display\n",
    "    let mut actual_y_intercept = constant_term / y_coefficient;\n",
    "    let actual_slope = -1.0 * x_coefficient / y_coefficient;\n",
    "    let mut actual_sign = \" \";\n",
    "    if actual_y_intercept > 0.0 {\n",
    "        actual_sign = \"+\";\n",
    "    } else {\n",
    "        actual_sign = \"-\";\n",
    "        actual_y_intercept *= -1.0;\n",
    "    }\n",
    "    let mut calculated_y_intercept = -perceptron.bias / perceptron.weights[1];\n",
    "    let calculated_slope = -perceptron.weights[0] / perceptron.weights[1];\n",
    "    let mut calculated_sign = \" \";\n",
    "    if calculated_y_intercept > 0.0 {\n",
    "        calculated_sign = \"+\";\n",
    "    } else {\n",
    "        calculated_sign = \"-\";\n",
    "        calculated_y_intercept *= -1.0;\n",
    "    }\n",
    "    \n",
    "    // show the results!\n",
    "    let mut x_value = x_coefficient;\n",
    "    let mut y_value = y_coefficient;\n",
    "    let mut c_term = constant_term;\n",
    "    let mut xplusy_sign = \"+\";\n",
    "    if y_value < 0.0 {\n",
    "        y_value *= -1.0;\n",
    "        xplusy_sign = \"-\";\n",
    "    }\n",
    "    let mut yplusc_sign = \"+\";\n",
    "    if c_term < 0.0 {\n",
    "        c_term *= -1.0;\n",
    "        yplusc_sign = \"-\";\n",
    "    }\n",
    "    println!(\"Original inequality: {:.2}x {} {:.2}y {} {:2}c > 0\", \n",
    "        x_value, xplusy_sign, y_value, yplusc_sign, c_term);\n",
    "    println!(\"The actual slope/intercept form : y = {:.2}x {} {:.2}\", \n",
    "        actual_slope, actual_sign, actual_y_intercept);\n",
    "    println!(\"Calculated slope/intercept form : y = {:.2}x {} {:.2}\", \n",
    "        calculated_slope, calculated_sign, calculated_y_intercept);\n",
    "    \n",
    "    \n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, before training Perceptron { bias: 0.01, learning_rate: 0.0005, weights: [0.6077100286095708, 0.041877317411672804] }\n",
      "After training Perceptron { bias: -0.2105000000000002, learning_rate: 0.0005, weights: [0.022081801615054946, -0.08212194192453111] }\n",
      "Original inequality: 1.40x - 5.00y + 13c > 0\n",
      "The actual slope/intercept form : y = 0.28x - 2.60\n",
      "Calculated slope/intercept form : y = 0.27x - 2.56\n"
     ]
    }
   ],
   "source": [
    "/// In this example, the perceptron solves for the inequality 1.4x -5y - 13 > 0')\n",
    "run(1.4, -5.0, 13.0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Rust",
   "language": "rust",
   "name": "rust"
  },
  "language_info": {
   "codemirror_mode": "rust",
   "file_extension": ".rs",
   "mimetype": "text/rust",
   "name": "Rust",
   "pygment_lexer": "rust",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
